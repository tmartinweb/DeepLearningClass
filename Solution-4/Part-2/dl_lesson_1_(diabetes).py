# -*- coding: utf-8 -*-
"""DL_Lesson_1_(diabetes).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sIVMHU-Q299fWgzJgvQGUIjrPvMZxK49
"""

import keras
from keras.models import Sequential
from keras.layers.core import Dense, Activation
from keras.initializers import Ones, Zeros, RandomNormal, RandomUniform, TruncatedNormal, LecunNormal
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder

from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

# Flag for running one dataset or the other (Diabetes vs. Breast Cancer)
diabetes = True  # (True for running first part, False for running second part)
standardize = False  # (True to run standardization, False for not)

if diabetes:
    # Read .csv file as Numpy Ndarray
    dataset = pd.read_csv('diabetes.csv', header=None).values

    # Print type and shape for reference
    print(type(dataset))
    print(dataset.shape)

    # Last column is our target [0, 1] **Diabetes**
    X_train, X_test, Y_train, Y_test = train_test_split(dataset[:, 0:8], dataset[:, 8], test_size=0.15, random_state=22)
    print("Using Diabetes Dataset")

else:
    # Read .csv file as DataFrame.
    dataset = pd.read_csv('Breast Cancer.csv')

    # Print type and shape for reference
    print(type(dataset))
    print(dataset.shape)

    dataset = dataset.dropna(axis=1)

    # Drop first column (ID), target in second column **Breast Cancer**
    dataset = dataset.drop(columns=["id"])
    targets = dataset.pop('diagnosis')

    # Categorize the Target from String to [0, 1]
    le = LabelEncoder().fit(targets)
    targets = le.transform(targets)

    # Split frame into X/Y Train/Test
    X_train, X_test, Y_train, Y_test = train_test_split(dataset.values, targets, test_size=0.15, random_state=22)
    print("Using Breast Cancer Dataset")

print("X_train.shape: ", X_train.shape)
print("Y_train.shape: ", Y_train.shape)
print("X_test.shape: ", X_test.shape)
print("Y_test.shape: ", Y_test.shape)

# Normalize/Standardize Data
if standardize:
    norm = StandardScaler().fit(X_train)
    X_train = norm.transform(X_train)
    X_test = norm.transform(X_test)

# create a Sequential model
my_first_nn = Sequential()

my_first_nn.add(Dense(8, input_dim=X_train.shape[1], activation='relu',
                      kernel_initializer=Ones(),
                      bias_initializer=RandomUniform())) # hidden layer
# Added Layers
my_first_nn.add(keras.layers.BatchNormalization())
my_first_nn.add(keras.layers.Dropout(.2))
my_first_nn.add(Dense(32, activation='elu',
                      kernel_initializer=Ones(),
                      bias_initializer=RandomUniform()))
my_first_nn.add(keras.layers.BatchNormalization())
my_first_nn.add(keras.layers.Dropout(.2))
my_first_nn.add(Dense(64, activation='elu',
                      kernel_initializer=Ones(),
                      bias_initializer=RandomUniform()))
my_first_nn.add(keras.layers.BatchNormalization())
my_first_nn.add(keras.layers.Dropout(.2))
my_first_nn.add(Dense(32, activation='elu',
                      kernel_initializer=Ones(),
                      bias_initializer=RandomUniform()))
my_first_nn.add(keras.layers.BatchNormalization())
my_first_nn.add(keras.layers.Dropout(.2))
my_first_nn.add(Dense(8, activation='relu',
                      kernel_initializer=Ones(),
                      bias_initializer=RandomUniform()))
my_first_nn.add(keras.layers.BatchNormalization())
my_first_nn.add(keras.layers.Dropout(.2))

# We should use the sigmoid activation function here because it is the best choice for binary classification
my_first_nn.add(Dense(1, activation='sigmoid')) # output layer (WHY 'sigmoid function!!!')

my_first_nn.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=.003), metrics=['acc'])

print(my_first_nn.summary())

# The returned history object holds a record of the loss values and metric values during training
history = my_first_nn.fit(X_train, Y_train, epochs=200, verbose=1, shuffle=True, validation_data=(X_test, Y_test))

# Plotting both training and validation accuracy from each epoch
plt.plot(history.history['acc'], label='train acc')
plt.plot(history.history['val_acc'], label='val acc')
plt.title('Model Training Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='lower right')
plt.show()

